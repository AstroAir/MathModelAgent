from app.utils.common_utils import transform_link, split_footnotes
from app.utils.log_util import logger
import asyncio
from app.schemas.response import (
    CoderMessage,
    WriterMessage,
    ModelerMessage,
    SystemMessage,
    CoordinatorMessage,
)
from app.services.redis_manager import redis_manager
from litellm import acompletion
import litellm
from app.schemas.enums import AgentType
from app.utils.track import agent_metrics
from icecream import ic
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from app.utils.provider_manager import ProviderManager

litellm.callbacks = [agent_metrics]


class LLM:
    def __init__(
        self,
        api_key: str,
        model: str,
        base_url: str,
        task_id: str,
    ):
        self.api_key = api_key
        self.model = model
        self.base_url = base_url
        self.chat_count = 0
        self.max_tokens: int | None = None  # æ·»åŠ æœ€å¤§tokenæ•°é™åˆ¶
        self.task_id = task_id

    async def chat(
        self,
        history: list = None,
        tools: list = None,
        tool_choice: str = None,
        max_retries: int = 8,  # æ·»åŠ æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: float = 1.0,  # æ·»åŠ é‡è¯•å»¶è¿Ÿ
        top_p: float | None = None,  # æ·»åŠ top_på‚æ•°,
        agent_name: AgentType = AgentType.SYSTEM,  # CoderAgent or WriterAgent
        sub_title: str | None = None,
    ) -> str:
        logger.info(f"subtitleæ˜¯:{sub_title}")

        # éªŒè¯å’Œä¿®å¤å·¥å…·è°ƒç”¨å®Œæ•´æ€§
        if history:
            history = self._validate_and_fix_tool_calls(history)

        kwargs = {
            "api_key": self.api_key,
            "model": self.model,
            "messages": history,
            "stream": False,
            "top_p": top_p,
            "metadata": {"agent_name": agent_name},
        }

        if tools:
            kwargs["tools"] = tools
            kwargs["tool_choice"] = tool_choice

        if self.max_tokens:
            kwargs["max_tokens"] = self.max_tokens

        if self.base_url:
            kwargs["base_url"] = self.base_url
        litellm.enable_json_schema_validation = True  # åŠ å…¥jsonæ ¼å¼éªŒè¯

        # å½“å‰ä½¿ç”¨éæµå¼è¾“å‡ºï¼Œç¡®ä¿å®Œæ•´å“åº”åå†å¤„ç†
        for attempt in range(max_retries):
            try:
                # completion = self.client.chat.completions.create(**kwargs)
                response = await acompletion(**kwargs)
                logger.info(f"APIè¿”å›: {response}")
                if not response or not hasattr(response, "choices"):
                    raise ValueError("æ— æ•ˆçš„APIå“åº”")
                self.chat_count += 1
                await self.send_message(response, agent_name, sub_title)
                return response
            except Exception as e:
                logger.error(
                    f"LLMè°ƒç”¨å¤±è´¥ï¼Œç¬¬{attempt + 1}/{max_retries}æ¬¡é‡è¯•: {str(e)}"
                )
                if attempt < max_retries - 1:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•
                    await asyncio.sleep(retry_delay * (attempt + 1))  # æŒ‡æ•°é€€é¿
                    continue
                logger.error(f"LLMè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {max_retries}")
                logger.debug(f"è¯·æ±‚å‚æ•°: {kwargs}")
                raise  # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸

    def _validate_and_fix_tool_calls(self, history: list) -> list:
        """éªŒè¯å¹¶ä¿®å¤å·¥å…·è°ƒç”¨å®Œæ•´æ€§"""
        if not history:
            return history

        ic(f"ğŸ” å¼€å§‹éªŒè¯å·¥å…·è°ƒç”¨ï¼Œå†å²æ¶ˆæ¯æ•°é‡: {len(history)}")

        # æŸ¥æ‰¾æ‰€æœ‰æœªåŒ¹é…çš„tool_calls
        fixed_history = []
        i = 0

        while i < len(history):
            msg = history[i]

            # å¦‚æœæ˜¯åŒ…å«tool_callsçš„æ¶ˆæ¯
            if isinstance(msg, dict) and "tool_calls" in msg and msg["tool_calls"]:
                ic(f"ğŸ“ å‘ç°tool_callsæ¶ˆæ¯åœ¨ä½ç½® {i}")

                # æ£€æŸ¥æ¯ä¸ªtool_callæ˜¯å¦éƒ½æœ‰å¯¹åº”çš„responseï¼Œåˆ†åˆ«å¤„ç†
                valid_tool_calls = []
                invalid_tool_calls = []

                for tool_call in msg["tool_calls"]:
                    tool_call_id = tool_call.get("id")
                    ic(f"  æ£€æŸ¥tool_call_id: {tool_call_id}")

                    if tool_call_id:
                        # æŸ¥æ‰¾å¯¹åº”çš„toolå“åº”
                        found_response = False
                        for j in range(i + 1, len(history)):
                            if (
                                history[j].get("role") == "tool"
                                and history[j].get("tool_call_id") == tool_call_id
                            ):
                                ic(f"  âœ… æ‰¾åˆ°åŒ¹é…å“åº”åœ¨ä½ç½® {j}")
                                found_response = True
                                break

                        if found_response:
                            valid_tool_calls.append(tool_call)
                        else:
                            ic(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…å“åº”: {tool_call_id}")
                            invalid_tool_calls.append(tool_call)

                # æ ¹æ®æ£€æŸ¥ç»“æœå¤„ç†æ¶ˆæ¯
                if valid_tool_calls:
                    # æœ‰æœ‰æ•ˆçš„tool_callsï¼Œä¿ç•™å®ƒä»¬
                    fixed_msg = msg.copy()
                    fixed_msg["tool_calls"] = valid_tool_calls
                    fixed_history.append(fixed_msg)
                    ic(
                        f"  ğŸ”§ ä¿ç•™ {len(valid_tool_calls)} ä¸ªæœ‰æ•ˆtool_callsï¼Œç§»é™¤ {len(invalid_tool_calls)} ä¸ªæ— æ•ˆçš„"
                    )
                else:
                    # æ²¡æœ‰æœ‰æ•ˆçš„tool_callsï¼Œç§»é™¤tool_callsä½†å¯èƒ½ä¿ç•™å…¶ä»–å†…å®¹
                    cleaned_msg = {k: v for k, v in msg.items() if k != "tool_calls"}
                    if cleaned_msg.get("content"):
                        fixed_history.append(cleaned_msg)
                        ic("  ğŸ”§ ç§»é™¤æ‰€æœ‰tool_callsï¼Œä¿ç•™æ¶ˆæ¯å†…å®¹")
                    else:
                        ic("  ğŸ—‘ï¸ å®Œå…¨ç§»é™¤ç©ºçš„tool_callsæ¶ˆæ¯")

            # å¦‚æœæ˜¯toolå“åº”æ¶ˆæ¯ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å­¤ç«‹çš„
            elif isinstance(msg, dict) and msg.get("role") == "tool":
                tool_call_id = msg.get("tool_call_id")
                ic(f"ğŸ”§ æ£€æŸ¥toolå“åº”æ¶ˆæ¯: {tool_call_id}")

                # æŸ¥æ‰¾å¯¹åº”çš„tool_calls
                found_call = False
                for j in range(len(fixed_history)):
                    if fixed_history[j].get("tool_calls") and any(
                        tc.get("id") == tool_call_id
                        for tc in fixed_history[j]["tool_calls"]
                    ):
                        found_call = True
                        break

                if found_call:
                    fixed_history.append(msg)
                    ic("  âœ… ä¿ç•™æœ‰æ•ˆçš„toolå“åº”")
                else:
                    ic(f"  ğŸ—‘ï¸ ç§»é™¤å­¤ç«‹çš„toolå“åº”: {tool_call_id}")

            else:
                # æ™®é€šæ¶ˆæ¯ï¼Œç›´æ¥ä¿ç•™
                fixed_history.append(msg)

            i += 1

        if len(fixed_history) != len(history):
            ic(f"ğŸ”§ ä¿®å¤å®Œæˆ: {len(history)} -> {len(fixed_history)} æ¡æ¶ˆæ¯")
        else:
            ic("âœ… éªŒè¯é€šè¿‡ï¼Œæ— éœ€ä¿®å¤")

        return fixed_history

    async def send_message(self, response, agent_name, sub_title=None):
        logger.info(f"subtitleæ˜¯:{sub_title}")
        content = response.choices[0].message.content

        match agent_name:
            case AgentType.CODER:
                agent_msg: CoderMessage = CoderMessage(content=content)
            case AgentType.WRITER:
                # å¤„ç† Markdown æ ¼å¼çš„å›¾ç‰‡è¯­æ³•
                content, _ = split_footnotes(content)
                content = transform_link(self.task_id, content)
                agent_msg: WriterMessage = WriterMessage(
                    content=content,
                    sub_title=sub_title,
                )
            case AgentType.MODELER:
                agent_msg: ModelerMessage = ModelerMessage(content=content)
            case AgentType.SYSTEM:
                agent_msg: SystemMessage = SystemMessage(content=content)
            case AgentType.COORDINATOR:
                agent_msg: CoordinatorMessage = CoordinatorMessage(content=content)
            case _:
                raise ValueError(f"ä¸æ”¯æŒçš„agentç±»å‹: {agent_name}")

        await redis_manager.publish_message(
            self.task_id,
            agent_msg,
        )


# class DeepSeekModel(LLM):
#     def __init__(
#         self,
#         api_key: str,
#         model: str,
#         base_url: str,
#         task_id: str,
#     ):
#         super().__init__(api_key, model, base_url, task_id)
# self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)


class ManagedLLM(LLM):
    """å¸¦æä¾›å•†ç®¡ç†å’Œé€Ÿç‡é™åˆ¶çš„ LLM"""

    def __init__(
        self,
        provider_manager: "ProviderManager",
        task_id: str,
        agent_name: str,
    ):
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªæä¾›å•†åˆå§‹åŒ–åŸºç±»
        first_provider = (
            provider_manager.providers[0] if provider_manager.providers else None
        )
        if not first_provider:
            raise ValueError(f"No providers configured for {agent_name}")

        super().__init__(
            api_key=first_provider.api_key,
            model=first_provider.model,
            base_url=first_provider.base_url,
            task_id=task_id,
        )

        self.provider_manager = provider_manager
        self.agent_name = agent_name
        self.current_provider = first_provider

    async def chat(
        self,
        history: list = None,
        tools: list = None,
        tool_choice: str = None,
        max_retries: int = 8,
        retry_delay: float = 1.0,
        top_p: float | None = None,
        agent_name: AgentType = AgentType.SYSTEM,
        sub_title: str | None = None,
    ) -> str:
        """
        å¸¦æä¾›å•†ç®¡ç†å’Œé€Ÿç‡é™åˆ¶çš„èŠå¤©æ–¹æ³•

        è‡ªåŠ¨å¤„ç†ï¼š
        - é€Ÿç‡é™åˆ¶æ£€æŸ¥
        - æä¾›å•†æ•…éšœè½¬ç§»
        - é‡è¯•é€»è¾‘
        """
        # ä¼°ç®— token æ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼šå­—ç¬¦æ•° / 4ï¼‰
        estimated_tokens = 0
        if history:
            for msg in history:
                content = msg.get("content", "")
                if isinstance(content, str):
                    estimated_tokens += len(content) // 4

        # å°è¯•æ‰€æœ‰å¯ç”¨çš„æä¾›å•†
        exclude_providers = []
        last_error = None

        for attempt in range(max_retries):
            try:
                # è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„æä¾›å•†
                provider = await self.provider_manager.get_next_provider(
                    estimated_tokens=estimated_tokens,
                    exclude_providers=exclude_providers,
                )

                if not provider:
                    logger.error(
                        "No available providers, all providers exhausted or rate limited"
                    )
                    if last_error:
                        raise last_error
                    raise Exception("No available providers")

                # æ›´æ–°å½“å‰æä¾›å•†é…ç½®
                self.current_provider = provider
                self.api_key = provider.api_key
                self.model = provider.model
                self.base_url = provider.base_url

                logger.info(
                    f"Using provider: {provider.name} (model: {provider.model}) "
                    f"for {self.agent_name}, attempt {attempt + 1}/{max_retries}"
                )

                # å‘é€é€Ÿç‡é™åˆ¶è­¦å‘Šæ¶ˆæ¯
                if attempt > 0:
                    await self._send_rate_limit_warning(provider.name, attempt)

                # è°ƒç”¨çˆ¶ç±»çš„ chat æ–¹æ³•
                try:
                    response = await super().chat(
                        history=history,
                        tools=tools,
                        tool_choice=tool_choice,
                        max_retries=1,  # å•ä¸ªæä¾›å•†åªå°è¯•ä¸€æ¬¡
                        retry_delay=retry_delay,
                        top_p=top_p,
                        agent_name=agent_name,
                        sub_title=sub_title,
                    )

                    # è®°å½•æˆåŠŸ
                    actual_tokens = 0
                    if hasattr(response, "usage") and response.usage:
                        actual_tokens = response.usage.total_tokens

                    await self.provider_manager.record_request_result(
                        provider=provider,
                        success=True,
                        actual_tokens=actual_tokens,
                        estimated_tokens=estimated_tokens,
                    )

                    return response

                except Exception as e:
                    last_error = e
                    logger.error(
                        f"Provider {provider.name} failed: {str(e)}, "
                        f"attempt {attempt + 1}/{max_retries}"
                    )

                    # è®°å½•å¤±è´¥
                    await self.provider_manager.record_request_result(
                        provider=provider,
                        success=False,
                    )

                    # å°†æ­¤æä¾›å•†åŠ å…¥æ’é™¤åˆ—è¡¨
                    exclude_providers.append(provider.get_identifier())

                    # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç»§ç»­
                    if attempt < max_retries - 1:
                        await asyncio.sleep(retry_delay * (attempt + 1))
                        continue
                    else:
                        raise

            except Exception as e:
                last_error = e
                if attempt < max_retries - 1:
                    logger.warning(f"Retrying after error: {str(e)}")
                    await asyncio.sleep(retry_delay * (attempt + 1))
                    continue
                else:
                    logger.error(f"All retry attempts exhausted: {str(e)}")
                    raise

        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        if last_error:
            raise last_error
        raise Exception("Failed to complete request after all retries")

    async def _send_rate_limit_warning(self, provider_name: str, attempt: int):
        """å‘é€é€Ÿç‡é™åˆ¶è­¦å‘Šæ¶ˆæ¯åˆ°å‰ç«¯"""
        warning_msg = SystemMessage(
            content=f"âš ï¸ é€Ÿç‡é™åˆ¶è§¦å‘ï¼Œåˆ‡æ¢åˆ°å¤‡ç”¨æä¾›å•†: {provider_name} (å°è¯• {attempt + 1})"
        )
        await redis_manager.publish_message(self.task_id, warning_msg)

    async def get_provider_stats(self):
        """è·å–æä¾›å•†ç»Ÿè®¡ä¿¡æ¯"""
        return await self.provider_manager.get_all_stats()


async def simple_chat(model: LLM, history: list) -> str:
    """
    Description of the function.

    Args:
        model (LLM): æ¨¡å‹
        history (list): æ„é€ å¥½çš„å†å²è®°å½•ï¼ˆåŒ…å«system_prompt,user_promptï¼‰

    Returns:
        return_type: Description of the return value.
    """
    kwargs = {
        "api_key": model.api_key,
        "model": model.model,
        "messages": history,
        "stream": False,
    }

    if model.base_url:
        kwargs["base_url"] = model.base_url

    response = await acompletion(**kwargs)

    return response.choices[0].message.content
